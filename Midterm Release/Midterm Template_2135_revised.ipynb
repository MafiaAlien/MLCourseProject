{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm\n",
    "\n",
    "Do not use scikit-learn API in section 1-3. \n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.io as scio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1 Regularized Linear Regression (20 pts)\n",
    "\n",
    "In the first half of the exam, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s.\n",
    "variance. \n",
    "\n",
    "### 1.1 Visualizing the dataset\n",
    "\n",
    "We will begin by visualizing the dataset containing historical records on the change in the water level, $x$, and the amount of water flowing out of the dam, $y$. This dataset is divided into three parts:\n",
    "\n",
    "- A **training** set that your model will learn on: `X`, `y`\n",
    "- A **cross validation** set for determining the regularization parameter: `Xval`, `yval`\n",
    "- A **test** set for evaluating performance. These are “unseen” examples which your model did not see during training: `Xtest`, `ytest`\n",
    "\n",
    "Run the next cell to plot the training data. In the following parts, you will implement linear regression and use that to fit a straight line to the data and plot learning curves. Following that, you will implement polynomial regression to find a better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load from data1.mat, where all variables will be store in a dictionary\n",
    "data = scio.loadmat('data1.mat')\n",
    "\n",
    "# Extract train, test, validation data from dictionary\n",
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "Xval = data['Xval']\n",
    "yval = data['yval'].flatten()\n",
    "Xtest = data['Xtest']\n",
    "ytest = data['ytest'].flatten()\n",
    "\n",
    "# m = Number of examples\n",
    "m = y.size\n",
    "\n",
    "# Plot training data\n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regularized linear regression cost function and gradient (10 pts)\n",
    "\n",
    "Recall that regularized linear regression has the following cost function:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\left( \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right) + \\frac{\\lambda}{2m} \\left( \\sum_{j=1}^n \\theta_j^2 \\right)$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter which controls the degree of regularization (thus, help preventing overfitting). The regularization term puts a penalty on the overall cost J. As the magnitudes of the model parameters $\\theta_j$ increase, the penalty increases as well. Note that you should not regularize\n",
    "the $\\theta_0$ term.\n",
    "\n",
    "You should now complete the code in the function `linearRegCostFunction` in the next cell. Your task is to calculate the regularized linear regression cost function. If possible, try to vectorize your code and avoid writing loops.\n",
    "<a id=\"linearRegCostFunction\"></a>\n",
    "\n",
    "\n",
    "\n",
    "Correspondingly, the partial derivative of the cost function for regularized linear regression is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the function [`linearRegCostFunction`](#linearRegCostFunction) below, add code to calculate the gradient, returning it in the variable `grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for regularized linear regression \n",
    "    with multiple variables. Computes the cost of using theta as\n",
    "    the parameter for linear regression to fit the data points in X and y. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset. Matrix with shape (m x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    theta : array_like\n",
    "        The parameters for linear regression. A vector of shape (n+1,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        The regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        The value of the cost function gradient w.r.t theta. \n",
    "        A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost and gradient of regularized linear regression for\n",
    "    a particular choice of theta.\n",
    "    You should set J to the cost and grad to the gradient.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    error = np.power(X.dot(theta.T)-y, 2)\n",
    "    regular = (lambda_ / (2*m)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n",
    "    J = (np.sum(error))/(2*m) + regular\n",
    "    parameter = int(theta.ravel().shape[1])\n",
    "\n",
    "    for j in range(parameter):\n",
    "        term = np.dot(error, X[:, j])\n",
    "        if j == 0 :\n",
    "            grad[j] = np.sum(term)\n",
    "        else:\n",
    "            grad[j] = np.sum(term)/(theta[:,j]*(lambda_/m))          \n",
    "    \n",
    "    # ============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished, the next cell will run your cost function using `theta` initialized at `[1, 1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta = [1, 1]:\t   7279.838623 \n",
      "Gradient at theta = [1, 1]:  [174715.126955, 11603422.669193] \n"
     ]
    }
   ],
   "source": [
    "theta = np.array([1, 1])\n",
    "\n",
    "J, grad = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)\n",
    "\n",
    "print('Cost at theta = [1, 1]:\\t   %f ' % J)\n",
    "\n",
    "print('Gradient at theta = [1, 1]:  [{:.6f}, {:.6f}] '.format(*grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fitting linear regression (10 pts)\n",
    "\n",
    "Next, you need to define `train_linear_reg` to compute the optimal values of $\\theta$. This training function uses `scipy`'s optimization module to minimize the cost function.\n",
    "\n",
    "In this part, we set regularization parameter $\\lambda$ to zero. Because our current implementation of linear regression is trying to fit a 2-dimensional $\\theta$, regularization will not be incredibly helpful for a $\\theta$ of such low dimension. In the later parts of the exercise, you will be using polynomial regression with regularization.\n",
    "\n",
    "Finally, the code in the next cell should also plot the best fit line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_reg(X, y, lambda_):\n",
    "    initial_theta = np.ones(X.shape[1])\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    res = opt.minimize(\n",
    "        linearRegCostFunction,\n",
    "        initial_theta,\n",
    "        args = (X, y, lambda_),\n",
    "        jac=True,\n",
    "        method=\"TNC\"\n",
    "    )\n",
    "    theta = res.x\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return theta\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (12,1) not aligned: 2 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-8c68bb68f790>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mX_aug\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mtheta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_linear_reg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_aug\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlambda_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;31m#  Plot fit over the data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mpyplot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'ro'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'k'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmew\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mpyplot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Change in water level (x)'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-7bd48673d5f3>\u001B[0m in \u001B[0;36mtrain_linear_reg\u001B[0;34m(X, y, lambda_)\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlambda_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mjac\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m         \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"TNC\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     )\n\u001B[1;32m     12\u001B[0m     \u001B[0mtheta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001B[0m in \u001B[0;36mminimize\u001B[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[1;32m    621\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'tnc'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    622\u001B[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001B[0;32m--> 623\u001B[0;31m                              **options)\n\u001B[0m\u001B[1;32m    624\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mmeth\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'cobyla'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    625\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_minimize_cobyla\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfun\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconstraints\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/tnc.py\u001B[0m in \u001B[0;36m_minimize_tnc\u001B[0;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, finite_diff_rel_step, maxfun, **unknown_options)\u001B[0m\n\u001B[1;32m    375\u001B[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001B[1;32m    376\u001B[0m                                   \u001B[0mfinite_diff_rel_step\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfinite_diff_rel_step\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 377\u001B[0;31m                                   bounds=new_bounds)\n\u001B[0m\u001B[1;32m    378\u001B[0m     \u001B[0mfunc_and_grad\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfun_and_grad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001B[0m in \u001B[0;36m_prepare_scalar_function\u001B[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001B[0m\n\u001B[1;32m    260\u001B[0m     \u001B[0;31m# calculation reduces overall function evaluations.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001B[0;32m--> 262\u001B[0;31m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001B[0m\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 136\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0;31m# Gradient evaluation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36m_update_fun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    224\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_update_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_updated\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 226\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_updated\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36mupdate_fun\u001B[0;34m()\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfun_wrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_fun_impl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001B[0m in \u001B[0;36mfun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mfun_wrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnfev\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mupdate_fun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     72\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0;34m\"\"\" returns the the function value \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_if_needed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Machine_learning_Winter_2021-2jFpTk8q/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001B[0m in \u001B[0;36m_compute_if_needed\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjac\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m             \u001B[0mfg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjac\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfg\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-44d4b0a56a58>\u001B[0m in \u001B[0;36mlinearRegCostFunction\u001B[0;34m(X, y, theta, lambda_)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[0mm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m     \u001B[0merror\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtheta\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m     \u001B[0mregular\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlambda_\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtheta\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mtheta\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[0mJ\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mregular\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (1,2) and (12,1) not aligned: 2 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "X_aug = np.c_[np.ones(m), X]\n",
    "theta = train_linear_reg(X_aug, y, lambda_)\n",
    "#  Plot fit over the data\n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1.5)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.plot(X, np.dot(X_aug, theta), '--', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit line tells us that the model is not a good fit to the data because the data has a non-linear pattern. While visualizing the best fit as shown is one possible way to debug your learning algorithm, it is not always easy to visualize the data and model. In the next section, you will implement a function to generate learning curves that can help you debug your learning algorithm even if it is not easy to visualize the\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 2 Bias-variance (10 pts)\n",
    "\n",
    "An important concept in machine learning is the bias-variance tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data.\n",
    "\n",
    "Next, you will plot training and testing errors on a learning curve to diagnose bias-variance problems.\n",
    "\n",
    "### 2.1 Learning Curves (10 pts)\n",
    "\n",
    "You will now implement code to generate the learning curves that will be useful in debugging learning algorithms. Recall that a learning curve plots training and cross validation error as a function of training set size. Your job is to fill in the function `learningCurve` in the next cell, so that it returns a vector of errors for the training set and cross validation set.\n",
    "\n",
    "To plot the learning curve, we need a training and cross validation set error for different training set sizes. To obtain different training set sizes, you should use different subsets of the original training set `X`. Specifically, for a training set size of $i$, you should use the first $i$ examples (i.e., `X[:i, :]`\n",
    "and `y[:i]`).\n",
    "\n",
    "You can use the `train_linear_reg` function to find the $\\theta$ parameters. Note that the `lambda_` is passed as a parameter to the `learningCurve` function.\n",
    "After learning the $\\theta$ parameters, you should compute the error on the training and cross validation sets. Recall that the training error for a dataset is defined as\n",
    "\n",
    "$$ J_{\\text{train}} = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left(h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right] $$\n",
    "\n",
    "In particular, note that the training error does not include the regularization term. One way to compute the training error is to use your existing cost function and set $\\lambda$ to 0 only when using it to compute the training error and cross validation error. When you are computing the training set error, make sure you compute it on the training subset (i.e., `X[:n,:]` and `y[:n]`) instead of the entire training set. However, for the cross validation error, you should compute it over the entire cross validation set. You should store\n",
    "the computed errors in the vectors error train and error val.\n",
    "\n",
    "<a id=\"func2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, lambda_=0):\n",
    "    \"\"\"\n",
    "    Generates the train and cross validation set errors needed to plot a learning curve\n",
    "    returns the train and cross validation set errors for a learning curve. \n",
    "    \n",
    "    In this function, you will compute the train and test errors for\n",
    "    dataset sizes from 1 up to m. In practice, when working with larger\n",
    "    datasets, you might want to do this in larger intervals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The training dataset. Matrix with shape (m x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each training datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    Xval : array_like\n",
    "        The validation dataset. Matrix with shape (m_val x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    yval : array_like\n",
    "        The functions values at each validation datapoint. A vector of\n",
    "        shape (m_val, ).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        The regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    error_train : array_like\n",
    "        A vector of shape m. error_train[i] contains the training error for\n",
    "        i examples.\n",
    "    error_val : array_like\n",
    "        A vecotr of shape m. error_val[i] contains the validation error for\n",
    "        i training examples.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return training errors in error_train and the\n",
    "    cross validation errors in error_val. i.e., error_train[i] and \n",
    "    error_val[i] should give you the errors obtained after training on i examples.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - You should evaluate the training error on the first i training\n",
    "      examples (i.e., X[:i, :] and y[:i]).\n",
    "    \n",
    "      For the cross-validation error, you should instead evaluate on\n",
    "      the _entire_ cross validation set (Xval and yval).\n",
    "    \n",
    "    - If you are using your cost function (linearRegCostFunction) to compute\n",
    "      the training and cross validation error, you should call the function with\n",
    "      the lambda argument set to 0. Do note that you will still need to use\n",
    "      lambda when running the training to obtain the theta parameters.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can loop over the examples with the following:\n",
    "     \n",
    "           for i in range(1, m+1):\n",
    "               # Compute train/cross validation errors using training examples \n",
    "               # X[:i, :] and y[:i], storing the result in \n",
    "               # error_train[i-1] and error_val[i-1]\n",
    "               ....  \n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "\n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m)\n",
    "    error_val   = np.zeros(m)\n",
    "    cost = np.zeros(iters)\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    theta =  np.zeros(1, X.shape[1])  \n",
    "    for i in range(len(X)):\n",
    "        theta = train_linear_reg(X[i], y[i], lambda_)\n",
    "        er_val_train =np.power((np.multiply(X[i], theta.T)-y[i]),2)\n",
    "        er_val_test =np.power((np.multiply(Xval[i], theta.T)-yval[i]),2)\n",
    "        error_train[i] = er_val_train\n",
    "        error_val[i] = er_val_test\n",
    "    # =============================================================\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished implementing the function `learningCurve`, executing the next cell prints the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "Xval_aug = np.concatenate([np.ones((yval.size, 1)), Xval], axis=1)\n",
    "error_train, error_val = learningCurve(X_aug, y, Xval_aug, yval, lambda_=0)\n",
    "\n",
    "pyplot.plot(np.arange(1, m+1), error_train, np.arange(1, m+1), error_val, lw=2)\n",
    "pyplot.title('Learning curve for linear regression')\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 150])\n",
    "\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the learning curve figure, you can observe that both the train error and cross validation error are high when the number of training examples is increased. This reflects a high bias problem in the model - the linear regression model is too simple and is unable to fit our dataset well. In the next section, you will implement polynomial regression to fit a better model for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "\n",
    "## 3 Polynomial regression (30 pts)\n",
    "\n",
    "The problem with our linear model was that it was too simple for the data\n",
    "and resulted in underfitting (high bias). Next, you will address this problem by adding more features. For polynomial regression, our hypothesis has the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x)  &= \\theta_0 + \\theta_1 \\times (\\text{waterLevel}) + \\theta_2 \\times (\\text{waterLevel})^2 + \\cdots + \\theta_p \\times (\\text{waterLevel})^p \\\\\n",
    "& = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_p x_p\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that by defining $x_1 = (\\text{waterLevel})$, $x_2 = (\\text{waterLevel})^2$ , $\\cdots$, $x_p =\n",
    "(\\text{waterLevel})^p$, we obtain a linear regression model where the features are the various powers of the original value (waterLevel).\n",
    "\n",
    "Now, you will add more features using the higher powers of the existing feature $x$ in the dataset. Your task in this part is to complete the code in the function `polyFeatures` in the next cell. The function should map the original training set $X$ of size $m \\times 1$ into its higher powers. Specifically, when a training set $X$ of size $m \\times 1$ is passed into the function, the function should return a $m \\times p$ matrix `X_poly`, where column 1 holds the original values of X, column 2 holds the values of $X^2$, column 3 holds the values of $X^3$, and so on. Note that you don’t have to account for the zero-eth power in this function.\n",
    "\n",
    "<a id=\"polyFeatures\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Polynomial Features (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, p):\n",
    "    \"\"\"\n",
    "    Maps X (1D vector) into the p-th power.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        A data vector of size m, where m is the number of examples.\n",
    "    \n",
    "    p : int\n",
    "        The polynomial power to map the features. \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    X_poly : array_like\n",
    "        A matrix of shape (m x p) where p is the polynomial \n",
    "        power and m is the number of examples. That is:\n",
    "    \n",
    "        X_poly[i, :] = [X[i], X[i]**2, X[i]**3 ...  X[i]**p]\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Given a vector X, return a matrix X_poly where the p-th column of\n",
    "    X contains the values of X to the p-th power.\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    X_poly = np.zeros((X.shape[0], p))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    degree = p\n",
    "    \n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the function `polyFeatures`, we will proceed to train polynomial regression using your linear regression cost function.\n",
    "\n",
    "Keep in mind that even though we have polynomial terms in our feature vector, we are still solving a linear regression optimization problem. The polynomial terms have simply turned into features that we can use for linear regression. We are using the same cost function and gradient that you wrote for the earlier part of this exercise.\n",
    "\n",
    "For this part of the exercise, you will be using a polynomial of degree 8. It turns out that if we run the training directly on the projected data, will not work well as the features would be badly scaled (e.g., an example with $x = 40$ will now have a feature $x_8 = 40^8 = 6.5 \\times 10^{12}$). Therefore, you will\n",
    "need to use feature normalization.\n",
    "\n",
    "Before learning the parameters $\\theta$ for the polynomial regression, you need to define `feature_normalize` and normalize the features of the training set, storing the mu, sigma parameters separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Normalization (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    X_norm = (X - X.mean()) / X.std()\n",
    "    mu = X.mean()\n",
    "    sigma = X.std()\n",
    "    # ============================================================\n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a function that will map features to a higher dimension. The next cell will apply it to the training set, the test set, and the cross validation set. You also need to normalize all three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 8\n",
    "\n",
    "# Map X onto Polynomial Features and Normalize\n",
    "X_poly = polyFeatures(X, p)\n",
    "X_poly, mu, sigma = feature_normalize(X_poly)\n",
    "X_poly = np.concatenate([np.ones((m, 1)), X_poly], axis=1)\n",
    "\n",
    "# Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p)\n",
    "X_poly_test -= mu\n",
    "X_poly_test /= sigma\n",
    "X_poly_test = np.concatenate([np.ones((ytest.size, 1)), X_poly_test], axis=1)\n",
    "\n",
    "# Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p)\n",
    "X_poly_val -= mu\n",
    "X_poly_val /= sigma\n",
    "X_poly_val = np.concatenate([np.ones((yval.size, 1)), X_poly_val], axis=1)\n",
    "\n",
    "print('Normalized Training Example 1:')\n",
    "X_poly[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Learning Polynomial Regression\n",
    "\n",
    "\n",
    "\n",
    "After learning the parameters $\\theta$, you should see two plots generated for polynomial regression with $\\lambda = 0$, which should be similar to the ones here:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/polynomial_regression.png\"></td>\n",
    "        <td><img src=\"Figures/polynomial_learning_curve.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "You should see that the polynomial fit is able to follow the datapoints very well, thus, obtaining a low training error. The figure on the right shows that the training error essentially stays zero for all numbers of training samples. However, the polynomial fit is very complex and even drops off at the extremes. This is an indicator that the polynomial regression model is overfitting the training data and will not generalize well.\n",
    "\n",
    "To better understand the problems with the unregularized ($\\lambda = 0$) model, you can see that the learning curve  shows the same effect where the training error is low, but the cross validation error is high. There is a gap between the training and cross validation errors, indicating a high variance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(min_x, max_x, mu, sigma, theta, p):\n",
    "    x = np.arange(min_x - 15, max_x + 25, 0.05)\n",
    "\n",
    "    X_poly = polyFeatures(x, p)\n",
    "    X_poly -= mu\n",
    "    X_poly /= sigma\n",
    "\n",
    "    X_poly = np.c_[np.ones(x.size), X_poly]\n",
    "\n",
    "    pyplot.plot(x, np.dot(X_poly, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "theta = train_linear_reg(X_poly, y,\n",
    "                             lambda_=lambda_)\n",
    "\n",
    "# Plot training data and fit\n",
    "pyplot.plot(X, y, 'ro', ms=10, mew=1.5, mec='k')\n",
    "\n",
    "plot_fit(np.min(X), np.max(X), mu, sigma, theta, p)\n",
    "\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.title('Polynomial Regression Fit (lambda = %f)' % lambda_)\n",
    "pyplot.ylim([-20, 50])\n",
    "\n",
    "pyplot.figure()\n",
    "error_train, error_val = learningCurve(X_poly, y, X_poly_val, yval, lambda_)\n",
    "pyplot.plot(np.arange(1, 1+m), error_train, np.arange(1, 1+m), error_val)\n",
    "\n",
    "pyplot.title('Polynomial Regression Learning Curve (lambda = %f)' % lambda_)\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 100])\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "\n",
    "print('Polynomial Regression (lambda = %f)\\n' % lambda_)\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to combat the overfitting (high-variance) problem is to add regularization to the model. In the next section, you will get to  try different $\\lambda$ parameters to see how regularization can lead to a better model.\n",
    "\n",
    "### 3.4 Optional (bonus) exercise: Adjusting the regularization parameter (5 pts)\n",
    "\n",
    "In this section, you will get to observe how the regularization parameter affects the bias-variance of regularized polynomial regression. You should now modify the the lambda parameter and try $\\lambda = 1, 100$. For each of these values, the script should generate a polynomial fit to the data and also a learning curve.\n",
    "\n",
    "For $\\lambda = 1$, you should see a polynomial fit that follows the data trend well and a learning curve showing that both the cross validation and training error converge to a relatively low value. This shows the $\\lambda = 1$ regularized polynomial regression model does not have the high-bias or high-variance problems. In effect, it achieves a good trade-off between bias and variance.\n",
    "\n",
    "For $\\lambda = 100$, you should see a polynomial fit that does not follow the data well. In this case, there is too much regularization and the model is unable to fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 3.5 Selecting $\\lambda$ using a cross validation set (10 pts)\n",
    "\n",
    "From the previous parts, you observed that the value of $\\lambda$ can significantly affect the results of regularized polynomial regression on the training and cross validation set. In particular, a model without regularization ($\\lambda = 0$) fits the training set well, but does not generalize. Conversely, a model with too much regularization ($\\lambda = 100$) does not fit the training set and testing set well. A good choice of $\\lambda$ (e.g., $\\lambda = 1$) can provide a good fit to the data.\n",
    "\n",
    "In this section, you will implement an automated method to select the $\\lambda$ parameter. Concretely, you will use a cross validation set to evaluate how good each $\\lambda$ value is. After selecting the best $\\lambda$ value using the cross validation set, we can then evaluate the model on the test set to estimate\n",
    "how well the model will perform on actual unseen data. \n",
    "\n",
    "Your task is to complete the code in the function `validationCurve`. Specifically, you should should use the `train_linear_reg` function to train the model using different values of $\\lambda$ and compute the training error and cross validation error. You should try $\\lambda$ in the following range: {0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}.\n",
    "<a id=\"validationCurve\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationCurve(X, y, Xval, yval):\n",
    "    \"\"\"\n",
    "    Generate the train and validation errors needed to plot a validation\n",
    "    curve that we can use to select lambda_.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The training dataset. Matrix with shape (m x n) where m is the \n",
    "        total number of training examples, and n is the number of features \n",
    "        including any polynomial features.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each training datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    Xval : array_like\n",
    "        The validation dataset. Matrix with shape (m_val x n) where m is the \n",
    "        total number of validation examples, and n is the number of features \n",
    "        including any polynomial features.\n",
    "    \n",
    "    yval : array_like\n",
    "        The functions values at each validation datapoint. A vector of\n",
    "        shape (m_val, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lambda_vec : list\n",
    "        The values of the regularization parameters which were used in \n",
    "        cross validation.\n",
    "    \n",
    "    error_train : list\n",
    "        The training error computed at each value for the regularization\n",
    "        parameter.\n",
    "    \n",
    "    error_val : list\n",
    "        The validation error computed at each value for the regularization\n",
    "        parameter.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return training errors in `error_train` and\n",
    "    the validation errors in `error_val`. The vector `lambda_vec` contains\n",
    "    the different lambda parameters to use for each calculation of the\n",
    "    errors, i.e, `error_train[i]`, and `error_val[i]` should give you the\n",
    "    errors obtained after training with `lambda_ = lambda_vec[i]`.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    You can loop over lambda_vec with the following:\n",
    "    \n",
    "          for i in range(len(lambda_vec))\n",
    "              lambda = lambda_vec[i]\n",
    "              # Compute train / val errors when training linear \n",
    "              # regression with regularization parameter lambda_\n",
    "              # You should store the result in error_train[i]\n",
    "              # and error_val[i]\n",
    "              ....\n",
    "    \"\"\"\n",
    "    # Selected values of lambda (you should not change this)\n",
    "    lambda_vec = np.array([0., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n",
    "\n",
    "    # You need to return these variables correctly.\n",
    "    error_train = np.zeros(lambda_vec.size)\n",
    "    error_val = np.zeros(lambda_vec.size)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # ============================================================\n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code, the next cell will run your function and plot a cross validation curve of error v.s. $\\lambda$ that allows you select which $\\lambda$ parameter to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec, error_train, error_val = validationCurve(X_poly, y, X_poly_val, yval)\n",
    "\n",
    "pyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('lambda')\n",
    "pyplot.ylabel('Error')\n",
    "\n",
    "print('lambda\\t\\tTrain Error\\tValidation Error')\n",
    "for i in range(len(lambda_vec)):\n",
    "    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6  Optional (bonus): Computing test set error (5 pts)\n",
    "\n",
    "In the previous part, you implemented code to compute the cross validation error for various values of the regularization parameter $\\lambda$. However, to get a better indication of the model’s performance in the real world, it is important to evaluate the “final” model on a test set that was not used in any part of training (that is, it was neither used to select the $\\lambda$ parameters, nor to learn the model parameters $\\theta$). For this optional exercise, you should compute the test error using the best value of $\\lambda$ you found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Optional (bonus): Plotting learning curves with randomly selected examples (10 pts)\n",
    "\n",
    "In practice, especially for small training sets, when you plot learning curves to debug your algorithms, it is often helpful to average across multiple sets of randomly selected examples to determine the training error and cross validation error.\n",
    "\n",
    "Concretely, to determine the training error and cross validation error for $i$ examples, you should first randomly select $i$ examples from the training set and $i$ examples from the cross validation set. You will then learn the parameters $\\theta$ using the randomly chosen training set and evaluate the parameters $\\theta$ on the randomly chosen training set and cross validation set. The above steps should then be repeated multiple times (say 50) and the averaged error should be used to determine the training error and cross validation error for $i$ examples.\n",
    "\n",
    "For this optional question, you should implement the above strategy for computing the learning curves. For reference, the figure below  shows the learning curve we obtained for polynomial regression with $\\lambda = 0.01$. Your figure may differ slightly due to the random selection of examples.\n",
    "\n",
    "![](Figures/learning_curve_random.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 Scikit-learn (40 pts)\n",
    "\n",
    "In this part, you need to use scikit-learn (pipeline, GridSearchCV, etc.) to replicate section 3 (add polynomial features, normalize using StandardScaler, and run ridge regression) and pick the best ridge regressor by choosing the best $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(8).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = poly.transform(X)\n",
    "Xval_poly = poly.transform(Xval)\n",
    "Xtest_poly = poly.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.fit(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = data['X']\n",
    "y = data['y'].flatten()\n",
    "Xval = data['Xval']\n",
    "yval = data['yval'].flatten()\n",
    "Xtest = data['Xtest']\n",
    "ytest = data['ytest'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = s.transform(X_poly)\n",
    "Xval_poly = s.transform(Xval_poly)\n",
    "Xtest_poly = s.transform(Xtest_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"alpha\":[0.001, 0.01, 0.1, 1, 10],\n",
    "    \"max_iter\": [i for i in range(100, 1001, 100)]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(Ridge(), param_grid=param_grid, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}